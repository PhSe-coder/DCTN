{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a4a747e3894816aa7d0acbb09142ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-08 13:25:26 INFO: Downloading default packages for language: en (English) ...\n",
      "2023-12-08 13:25:27 INFO: File exists: /root/stanza_resources/en/default.zip\n",
      "2023-12-08 13:25:31 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
      "2023-12-08 13:25:32 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2023-12-08 13:25:32 INFO: Using device: cuda\n",
      "2023-12-08 13:25:32 INFO: Loading: tokenize\n",
      "2023-12-08 13:25:32 INFO: Loading: pos\n",
      "2023-12-08 13:25:33 INFO: Loading: lemma\n",
      "2023-12-08 13:25:33 INFO: Loading: depparse\n",
      "2023-12-08 13:25:33 INFO: Done loading processors!\n",
      "laptop: 100%|██████████| 3045/3045 [00:00<00:00, 8023.59it/s]\n",
      "restaurant: 100%|██████████| 3877/3877 [00:00<00:00, 8714.12it/s]\n",
      "twitter: 100%|██████████| 6248/6248 [00:00<00:00, 6636.02it/s]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import os.path as osp\n",
    "from stanza_utils import *\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "pos, dep, tag = {}, {}, {}\n",
    "sentis = {\n",
    "    \"1\": \"T-POS\",\n",
    "    \"0\": \"T-NEU\",\n",
    "    \"-1\": \"T-NEG\"\n",
    "}\n",
    "for file in glob(\"../data/raw/**.train.txt\"):\n",
    "    domain = osp.basename(file).split('.')[0]\n",
    "    pos_list, dep_list, tag_list = [], [], []\n",
    "    lines = open(file, \"r\").read().splitlines()\n",
    "    # if domain == \"twitter\":\n",
    "    #     ans = []\n",
    "    #     for line, aspect, polarity in zip(lines[::3], lines[1::3], lines[2::3]):\n",
    "    #         text = line.replace(\"$T$\", aspect)\n",
    "    #         words = line.split(' ')\n",
    "    #         anns = ['O'] * len(words)\n",
    "    #         for i, word in enumerate(words):\n",
    "    #             if word != \"$T$\": continue\n",
    "    #             anns[i] = ' '.join([sentis[polarity]] * len(aspect.split(' ')))\n",
    "    #         assert len(words) == len(anns)\n",
    "    #         ans.append(f\"{text}***{' '.join(anns)}\")\n",
    "    #     lines = ans\n",
    "    sentences_list = annotation_plus([line.rsplit(\"***\", maxsplit=1)[0] for line in lines])\n",
    "    label_list = [line.rsplit(\"***\", maxsplit=1)[1] for line in lines]\n",
    "    for sentence, labels in tqdm(zip(sentences_list, label_list), total=len(lines), desc=domain):\n",
    "        for token, label in zip(sentence.to_dict(), labels.split(' ')):\n",
    "            pos_list.append(token['xpos'])\n",
    "            dep_list.append(token['deprel'])\n",
    "            tag_list.append(label)\n",
    "    pos[domain] = pos_list\n",
    "    dep[domain] = dep_list\n",
    "    tag[domain] = tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    " \n",
    "domains = pos.keys()\n",
    "total = 0\n",
    "\n",
    "\n",
    "def func(x: pd.Series):\n",
    "    if x.name == \"tag\": return len(x[x != 'O']) * 1.0 / len(x)\n",
    "    else: return len(x[x != 'O']) * 1.0 / total\n",
    "\n",
    "\n",
    "writer = ExcelWriter(\"./annotation.xlsx\")\n",
    "pos_writer = ExcelWriter(\"./pos.xlsx\")\n",
    "dep_writer = ExcelWriter(\"./dep.xlsx\")\n",
    "for i, domain in enumerate(domains):\n",
    "    data = pd.DataFrame({\"pos\": pos[domain], \"dep\": dep[domain], \"tag\": tag[domain]})\n",
    "    total = len(data[data['tag'] != 'O'])\n",
    "    data['tag_count'] = data['tag'].copy()\n",
    "    groups = data.groupby([\"pos\", \"dep\"], group_keys=False)\n",
    "    df = groups.agg(func).sort_values([\"tag_count\", \"tag\"], ascending=False)\n",
    "    df.to_excel(writer, sheet_name=domain)\n",
    "    pos_df = df.groupby(\"pos\").agg({\n",
    "        \"tag_count\": \"sum\"\n",
    "    }).sort_values(\"tag_count\", ascending=False).head(15)\n",
    "    pos_df.to_excel(pos_writer, sheet_name=domain)\n",
    "    dep_df = df.groupby(\"dep\").agg({\n",
    "        \"tag_count\": \"sum\"\n",
    "    }).sort_values(\"tag_count\", ascending=False).head(15)\n",
    "    dep_df.to_excel(dep_writer, sheet_name=domain)\n",
    "writer.close()\n",
    "pos_writer.close()\n",
    "dep_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
